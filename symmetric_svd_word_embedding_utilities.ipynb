{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import preprocessor as p\n",
    "import string\n",
    "p.set_options(p.OPT.EMOJI,p.OPT.MENTION,p.OPT.URL)\n",
    "\n",
    "import csv\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from itertools import dropwhile\n",
    "from itertools import chain\n",
    "import operator\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tweet(tweet_list):\n",
    "    ## remove punctuations, strip punctuations from each token\n",
    "    ## expect input a list of strings: [\"RT @unitetheunion...\",\"Britain needs a ..\", ....\"proper pay..\"...]\n",
    "    ## expected output a list of seperate tokens [['RT','@unitetheunion'],....['proper','pay']]\n",
    "    \n",
    "    \n",
    "    clean_tweet=[]\n",
    "    text = [p.clean(i).lower() for i in tweet_list]\n",
    "    text = [i.split(' ') for i in text]\n",
    "    \n",
    "    for tweet in text:\n",
    "        temp_list=[]\n",
    "        for token in tweet:\n",
    "            temp_word = token.strip(string.punctuation)\n",
    "            temp_list.append(temp_word)\n",
    "        clean_tweet.append(temp_list)\n",
    "            \n",
    "    return clean_tweet\n",
    "\n",
    "def word_counter(tweet_list):\n",
    "    ## counter the frequency of token\n",
    "    ## expected input a list of seperate tokens [['RT','@unitetheunion'//],....['proper','pay'..]]\n",
    "    ## expected output a python Counter type storing the frequency of each token. e.g counter['hello']:1\n",
    "    \n",
    "    counter_ = Counter()\n",
    "    for tweet in tweet_list:\n",
    "        counter_.update(tweet)\n",
    "        \n",
    "    return counter_\n",
    "\n",
    "def remove_rare_stop_words(tweet_set,counter,num,stop):\n",
    "    ## remove stop words and rare terms which frequency less than certain words\n",
    "    ## expected input a list of tokens [['RT','@unitetheunion'],....['proper','pay']], counter storing individual\n",
    "    ## words frequency, threshold of the rare term, a list of stop words ['i', 'me', 'my', 'myself',.. , '&amp', 'a', '', 'amp']\n",
    "    ## expected output a list of seperate tokens without stop words and rare tokens [['@unitetheunion'..],....['proper','pay'..]]\n",
    "    \n",
    "    stop_words = stop\n",
    "    \n",
    "    rare_terms=[]\n",
    "    for key in counter:\n",
    "        if counter[key]<=num:\n",
    "            rare_terms.append(key)        \n",
    "            \n",
    "    ban_list = set(stop_words + rare_terms)\n",
    "    \n",
    "    clean_tweet_remove_stop = []\n",
    "    for tweet in tweet_set:\n",
    "        temp = []\n",
    "        [temp.append(term) for term in tweet if term not in ban_list]\n",
    "        clean_tweet_remove_stop.append(temp)\n",
    "    \n",
    "    return clean_tweet_remove_stop\n",
    "\n",
    "def collocation_counter(tweet_list,order=False):\n",
    "    ## count the co-occurrences of any two words\n",
    "    ## expected input a list of seperated cleaned tokens [['@unitetheunion'..],....['proper','pay'..]]\n",
    "    ## a order flag, default is False, that means ignoring the order, it will treat ['a','b'] and ['b','a'] the same\n",
    "    ## expected output a counter storing the cooccurrences of bigrams e.g. counter[('hellow','world)]:17\n",
    "    \n",
    "\n",
    "    if order == True:\n",
    "        count_collocation = Counter()\n",
    "        for tweet in tweet_list:\n",
    "            for idx,word in enumerate(tweet[:-1]):\n",
    "                for i in range(idx,len(tweet)-1):\n",
    "                    count_collocation.update(((word,tweet[i+1]),))\n",
    "\n",
    "\n",
    "\n",
    "        return count_collocation\n",
    "    else:\n",
    "        count_collocation = Counter()\n",
    "        for tweet in tweet_list:\n",
    "            for idx,word in enumerate(tweet[:-1]):\n",
    "                for i in range(idx,len(tweet)-1):\n",
    "                    if (tweet[i+1],word) in count_collocation:\n",
    "                        count_collocation.update(((tweet[i+1],word),))\n",
    "                    else:\n",
    "                        count_collocation.update(((word,tweet[i+1]),))\n",
    "\n",
    "\n",
    "        return count_collocation\n",
    "\n",
    "def remove_rare_term(counter_,num):\n",
    "    ## remove the low frequency bigrams from the bigram counter\n",
    "    ## expected input a counter of bigrams, number of frequency threshold\n",
    "    ## expected output a counter storing the bigrams with frequency higher than threshold, e.g. counter[('hellow','world)]:17\n",
    "    \n",
    "    counter = counter_.copy()\n",
    "    for key, count in dropwhile(lambda key_count: key_count[1] > num, counter.most_common()):\n",
    "        del counter[key]\n",
    "        \n",
    "    return counter\n",
    "  \n",
    "\n",
    "    \n",
    "def construct_word_occurrence_matrix(counter_uni,counter_col):\n",
    "    \n",
    "    ## construct a word co-occurrence matrix\n",
    "    ## expected input, a counter storing uni-gram frequency, a counter storing bigram frequency\n",
    "    ## expected output, a pandas dataframe of co-occurences, and a np array values, shape is the (len(vocab),len(vocab))\n",
    "     \n",
    "    ##      A\tB\tC\tD\n",
    "    ##   A\t0.0\t2.0\t1.0\t1.0\n",
    "    ##   B\t2.0\t0.0\t2.0\t1.0\n",
    "    ##   C\t1.0\t2.0\t0.0\t1.0\n",
    "    ##   D\t1.0\t1.0\t1.0\t0.0\n",
    "    \n",
    "    vocab_size = len(counter_uni)\n",
    "    header = counter_uni.keys()\n",
    "    a = np.zeros((vocab_size,vocab_size))\n",
    "    frame = pd.DataFrame(a,index=header,columns=header)\n",
    "    \n",
    "    for item in counter_col:\n",
    "        frame[item[0]][item[1]] = counter_col[item]\n",
    "        frame[item[1]][item[0]] = counter_col[item]\n",
    "        \n",
    "    return frame,frame.values\n",
    "    \n",
    "    \n",
    "def PPMI_matrix(names_index,names_header,matrix,la_smooth=False):\n",
    "    \n",
    "    ## construct a positive-pmi matrix\n",
    "    ## expected input, the index, header, usually is a list of vocabulary ['allen','apple','banana',..'zoo']\n",
    "    ## the word co-occurrence matrix, laplace smooth value, default is false, expected smooth range from 0-3\n",
    "    ## expected output a PPMI matrix\n",
    "    \n",
    "    ##    \t   A\tB\tC\tD\n",
    "    ## A\t0.000000\t0.678072\t0.000000\t0.415037\n",
    "    ## B\t0.678072\t0.000000\t0.678072\t0.093109\n",
    "    ## C\t0.000000\t0.678072\t0.000000\t0.415037\n",
    "    ## D\t0.415037\t0.093109\t0.415037\t0.000000\n",
    "    \n",
    "    if la_smooth:\n",
    "        print('applying laplace smoothed, parameter is', la_smooth)\n",
    "        \n",
    "        matrix_sm = matrix.copy()\n",
    "        matrix_sm = matrix_sm + la_smooth\n",
    "        N = matrix_sm.sum()\n",
    "        \n",
    "    \n",
    "        num_row = matrix_sm.shape[0]\n",
    "        num_col = matrix_sm.shape[1]\n",
    "\n",
    "        col = matrix_sm.sum(0)\n",
    "        row = matrix_sm.sum(1)\n",
    "\n",
    "        row = row.reshape(num_row,1)\n",
    "        col = col.reshape(1,num_col)\n",
    "\n",
    "        denominator_matrix = np.dot(row,col)\n",
    "\n",
    "        pmi = np.log2(matrix_sm*N/denominator_matrix)\n",
    "\n",
    "        pmi[np.isnan(pmi)]=0\n",
    "        pmi[pmi<0]=0\n",
    "\n",
    "        dff = pd.DataFrame(pmi,index=names_index,columns=names_header)\n",
    "        \n",
    "    else:\n",
    "        print('no smooth applied')\n",
    "        N = matrix.sum()\n",
    "\n",
    "        num_row = matrix.shape[0]\n",
    "        num_col = matrix.shape[1]\n",
    "\n",
    "        col = matrix.sum(0)\n",
    "        row = matrix.sum(1)\n",
    "\n",
    "        row = row.reshape(num_row,1)\n",
    "        col = col.reshape(1,num_col)\n",
    "\n",
    "        denominator_matrix = np.dot(row,col)\n",
    "\n",
    "        pmi = np.log2(matrix*N/denominator_matrix)\n",
    "\n",
    "        pmi[np.isnan(pmi)]=0\n",
    "        pmi[pmi<0]=0\n",
    "\n",
    "        dff = pd.DataFrame(pmi,index=names_index,columns=names_header)\n",
    "    \n",
    "    return dff,pmi\n",
    "    \n",
    "\n",
    "def construct_word_embedding(d,matrix):\n",
    "    \n",
    "    ## use SVD factorization a PMI matrix\n",
    "    ## expected input, the dimenstion of output matrix, the PPMI matrix\n",
    "    ## expected output, the symmetrics matrix W, C , shape is (len(vocab),d)\n",
    "    \n",
    "    ## [[-5.29823596e-01  5.29767001e-01 ]  assume input a (4,4) matrix,we set dimension 2\n",
    "    ## [-6.49736688e-01 -6.32321728e-01 ]\n",
    "    ## [-5.29823596e-01  5.29767001e-01 ]\n",
    "    ## [-4.28595395e-01 -3.51201836e-01]] \n",
    "    \n",
    "    u, s, vh = np.linalg.svd(matrix, full_matrices=True)\n",
    "    \n",
    "    sigma = np.zeros((d,d))\n",
    "    sigma = np.diag(s[:d])\n",
    "    sigma = np.sqrt(sigma)\n",
    "    \n",
    "    W = np.dot(u[:,:d],sigma)\n",
    "    C = np.dot(vh[:,:d],sigma)\n",
    "    \n",
    "    return W,C\n",
    "    \n",
    "    \n",
    "def to_csv(matrix,header,filename):\n",
    "    \n",
    "    ## export the word embedding dataframe to a csv file and return the dataframe\n",
    "    ## expected input, the embedding matrix, the index of dataframe, the filename for outputint\n",
    "    \n",
    "    frame = pd.DataFrame(matrix,index=header)\n",
    "    frame.to_csv(filename)\n",
    "    \n",
    "    return frame\n",
    "    \n",
    "def read_from_csv(mypath,header_row):\n",
    "    \n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    print('the filenames are ***************************')\n",
    "    print(onlyfiles)\n",
    "    \n",
    "    df_raw = pd.DataFrame(columns=header_row)\n",
    "    for idx,filename in enumerate(onlyfiles):\n",
    "        df = pd.read_csv(mypath+'/'+filename,names=header_row)\n",
    "        frames = [df_raw,df]\n",
    "        df_raw = pd.concat(frames)\n",
    "        \n",
    "    print('Original size of data is ', len(df_raw))\n",
    "    df_raw = df_raw.drop_duplicates(subset='text')\n",
    "    df_raw = df_raw.reset_index(drop=True)\n",
    "    print('after remove duplicate, the size is ',len(df_raw))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    return df_raw\n",
    "    \n",
    "def read_from_txt(mypath):\n",
    "    onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    print('the filenames are ***************************')\n",
    "    print(onlyfiles)\n",
    "    \n",
    "    df_raw = pd.DataFrame()\n",
    "    for idx,filename in enumerate(onlyfiles):\n",
    "        df = pd.read_csv(mypath+'/'+filename,sep=\" \")\n",
    "        frames = [df_raw,df]\n",
    "        df_raw = pd.concat(frames)\n",
    "        \n",
    "    print('Original size of data is ', len(df_raw))\n",
    "    df_raw = df_raw.drop_duplicates(subset='text')\n",
    "    df_raw = df_raw.reset_index(drop=True)\n",
    "    print('after remove duplicate, the size is ',len(df_raw))\n",
    "    print(\"\\n\")\n",
    "\n",
    "       \n",
    "    return df_raw\n",
    "\n",
    "def co_occurrence_matrix(document):\n",
    "    \n",
    "    ## another way for constructing co_occurrence matrix, P.S. this method requiring large memory \n",
    "    ## expected input a list of seperated cleaned tokens [['@unitetheunion'..],....['proper','pay'..]]\n",
    "    ## expected output the word co-occurrences matrix\n",
    "    \n",
    "    names = sorted(set(list(chain(*document))))\n",
    "    voc2id = dict(zip(names, range(len(names))))\n",
    "    rows, cols, vals = [], [], []\n",
    "    for r, d in enumerate(document):\n",
    "        for e in d:\n",
    "            if voc2id.get(e) is not None:\n",
    "                rows.append(r)\n",
    "                cols.append(voc2id[e])\n",
    "                vals.append(1)\n",
    "    X = sp.csr_matrix((vals, (rows, cols)))\n",
    "    \n",
    "    Xc = (X.T * X) # coocurrence matrix\n",
    "    Xc.setdiag(0)\n",
    "    a = Xc.toarray()\n",
    "    \n",
    "    dff = pd.DataFrame(a,index = names, columns=names)\n",
    "    \n",
    "    return dff,a\n",
    "\n",
    "\n",
    "def penalty_frequency_words():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
